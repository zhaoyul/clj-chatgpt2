#!/usr/bin/env python3
"""
GPT-2 æ¨¡å‹è®¾ç½®è„šæœ¬
è‡ªåŠ¨ä¸‹è½½å¹¶å¯¼å‡º ONNX æ¨¡å‹

ç”¨æ³•:
  python scripts/setup_model.py [--model gpt2] [--output resources/onnx/]
"""

import argparse
import os
import sys
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    required = {
        'transformers': 'transformers>=4.30.0',
        'torch': 'torch>=2.0.0',
        'onnx': 'onnx>=1.14.0',
    }
    
    missing = []
    for pkg, install_name in required.items():
        try:
            __import__(pkg)
        except ImportError:
            missing.append(install_name)
    
    if missing:
        print("âŒ ç¼ºå°‘ä¾èµ–åŒ…:")
        for pkg in missing:
            print(f"   - {pkg}")
        print("\nè¯·å®‰è£…ä¾èµ–:")
        print(f"  pip install {' '.join(missing)}")
        sys.exit(1)
    
    import transformers
    import torch
    print(f"âœ… transformers {transformers.__version__}")
    print(f"âœ… torch {torch.__version__}")

def download_tokenizer(model_name: str, output_dir: Path):
    """ä¸‹è½½åˆ†è¯å™¨é…ç½®æ–‡ä»¶"""
    from transformers import GPT2Tokenizer
    
    print(f"\nğŸ“¥ æ­£åœ¨ä¸‹è½½åˆ†è¯å™¨: {model_name}")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ… åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {output_dir}")
    
    # åˆ—å‡ºä¸‹è½½çš„æ–‡ä»¶
    for f in output_dir.iterdir():
        if f.is_file():
            size = f.stat().st_size
            print(f"   - {f.name} ({size/1024:.1f} KB)" if size > 1024 else f"   - {f.name} ({size} B)")

def export_onnx(model_name: str, output_dir: Path):
    """å¯¼å‡º ONNX æ¨¡å‹"""
    import torch
    from transformers import GPT2LMHeadModel
    
    print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()
    
    # åˆ›å»ºåŒ…è£…å™¨
    class GPT2Wrapper(torch.nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
        
        def forward(self, input_ids, attention_mask):
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True,
                output_attentions=False,
                output_hidden_states=False
            )
            return outputs.logits
    
    wrapped_model = GPT2Wrapper(model)
    wrapped_model.eval()
    
    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    dummy_input_ids = torch.randint(0, 50257, (1, 10))
    dummy_attention_mask = torch.ones(1, 10, dtype=torch.long)
    
    # å¯¼å‡ºè·¯å¾„
    onnx_path = output_dir / "model.onnx"
    
    print(f"\nğŸ”§ æ­£åœ¨å¯¼å‡º ONNX æ¨¡å‹...")
    print(f"   è¾“å‡ºè·¯å¾„: {onnx_path}")
    
    # åŠ¨æ€è½´é…ç½®
    dynamic_axes = {
        "input_ids": {0: "batch_size", 1: "sequence_length"},
        "attention_mask": {0: "batch_size", 1: "sequence_length"},
        "logits": {0: "batch_size", 1: "sequence_length", 2: "vocab_size"}
    }
    
    torch.onnx.export(
        wrapped_model,
        (dummy_input_ids, dummy_attention_mask),
        str(onnx_path),
        input_names=["input_ids", "attention_mask"],
        output_names=["logits"],
        dynamic_axes=dynamic_axes,
        opset_version=14,
        do_constant_folding=True,
        verbose=False
    )
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
    size_mb = onnx_path.stat().st_size / 1024 / 1024
    print(f"âœ… ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸ!")
    print(f"   æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
    
    return str(onnx_path)

def verify_model(onnx_path: Path):
    """éªŒè¯å¯¼å‡ºçš„æ¨¡å‹"""
    try:
        import onnx
        import onnxruntime as ort
        
        print("\nğŸ” éªŒè¯ ONNX æ¨¡å‹...")
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        model = onnx.load(str(onnx_path))
        onnx.checker.check_model(model)
        print("   âœ… æ¨¡å‹ç»“æ„æ£€æŸ¥é€šè¿‡")
        
        # æµ‹è¯•æ¨ç†
        session = ort.InferenceSession(str(onnx_path))
        print(f"   âœ… è¾“å…¥: {[i.name for i in session.get_inputs()]}")
        print(f"   âœ… è¾“å‡º: {[o.name for o in session.get_outputs()]}")
        print("   âœ… ONNX Runtime å¯ä»¥æ­£å¸¸åŠ è½½æ¨¡å‹")
        
    except ImportError:
        print("   âš ï¸  æœªå®‰è£… onnx/onnxruntimeï¼Œè·³è¿‡éªŒè¯")
    except Exception as e:
        print(f"   âš ï¸  éªŒè¯å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½å¹¶å¯¼å‡º GPT-2 æ¨¡å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é»˜è®¤ä¸‹è½½ gpt2 åŸºç¡€ç‰ˆ (124M)
  python scripts/setup_model.py
  
  # ä¸‹è½½æ›´å¤§çš„æ¨¡å‹
  python scripts/setup_model.py --model gpt2-medium
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python scripts/setup_model.py --output /path/to/models
        """
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt2",
        choices=["gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"],
        help="æ¨¡å‹åç§° (é»˜è®¤: gpt2)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="resources/onnx",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: resources/onnx)"
    )
    parser.add_argument(
        "--skip-tokenizer",
        action="store_true",
        help="è·³è¿‡ä¸‹è½½åˆ†è¯å™¨"
    )
    parser.add_argument(
        "--skip-onnx",
        action="store_true",
        help="è·³è¿‡å¯¼å‡º ONNX æ¨¡å‹"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("GPT-2 æ¨¡å‹è®¾ç½®å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¸‹è½½åˆ†è¯å™¨
    if not args.skip_tokenizer:
        download_tokenizer(args.model, output_dir)
    
    # å¯¼å‡º ONNX
    if not args.skip_onnx:
        onnx_path = export_onnx(args.model, output_dir)
        verify_model(Path(onnx_path))
    
    print("\n" + "=" * 60)
    print("âœ… è®¾ç½®å®Œæˆ!")
    print("=" * 60)
    print(f"\næ¨¡å‹æ–‡ä»¶ä½ç½®: {output_dir}")
    print("\nä½ ç°åœ¨å¯ä»¥:")
    print("  1. è¿è¡Œæµ‹è¯•: clojure -M:test")
    print("  2. å¯åŠ¨æœåŠ¡: clojure -M -m gpt2.server")
    print("  3. å¯åŠ¨ Clerk: ./scripts/clerk.sh")

if __name__ == "__main__":
    main()
